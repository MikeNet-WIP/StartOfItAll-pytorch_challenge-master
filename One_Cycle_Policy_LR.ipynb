{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ff47687adb5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mOne_Cycle_Policy_LR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_LRScheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \"\"\"Starting with the optimizer's default learning rate, during a single cycle with the given cycle length, linearly\n\u001b[0;32m      3\u001b[0m     \u001b[0mincreases\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mrate\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmaximum\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mrate\u001b[0m \u001b[0mspecified\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mdecreases\u001b[0m \u001b[0mit\u001b[0m \u001b[0mback\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minitial\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mAfter\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcycle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mfinished\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mrate\u001b[0m \u001b[0mwill\u001b[0m \u001b[1;32mcontinue\u001b[0m \u001b[0mdecreasing\u001b[0m \u001b[0mlinearly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m \u001b[0mexponentially\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mremain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     constant depending on the mode specified. (The original implementation decreases the learning rate linearly to\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr_scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "class One_Cycle_Policy_LR(lr_scheduler._LRScheduler):\n",
    "    \"\"\"Starting with the optimizer's default learning rate, during a single cycle with the given cycle length, linearly\n",
    "    increases the learning rate to the maximum learning rate specified, then decreases it back to the initial value.\n",
    "    After the cycle is finished, the learning rate will continue decreasing linearly, decay exponentially, or remain\n",
    "    constant depending on the mode specified. (The original implementation decreases the learning rate linearly to\n",
    "    the annihilation)\n",
    "    \n",
    "    Optionally, also controls the momentum value; decreasing and increasing it during the cycle.\n",
    "    \n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        max_lr (int or float): Maximum learning rate.\n",
    "        cycle_length (int or float): Number of epochs for the cycle. The value\n",
    "            can be a float as well.\n",
    "        total_epochs (int or float): Total number of epochs during the training.\n",
    "            Used only if `lr_mode_after_cycle` is set to `linear`. Can be a float.\n",
    "            Otherwise, the value is ignored. Should be greater than `cycle_length`,\n",
    "            though not strictly prohibited. Default: 0.\n",
    "        gamma (int or float): Multiplicative factor of learning rate decay. Used only if\n",
    "            `lr_mode_after_cycle` is set to `exponential`. Ignored for other modes.\n",
    "            Default: 0.98.\n",
    "        const_lr (int or float): Constant learning rate value. Used only if `lr_mode_after_cycle`\n",
    "            is set to 'constant'. Ignored for other modes. Default: 1e-03.\n",
    "        min_lr (int or float): Minimum learning rate after the cycle. Default: 0.\n",
    "        lr_mode_after_cycle (str): One of `linear`, `exponential`, `constant`.\n",
    "            In `linear` mode, the learning rate will continue to decrease linearly\n",
    "            towards 0 after the cycle is finished until `total_epochs` specified, and it\n",
    "            will be 0 after it reaches `total_epochs`. In `exponential` mode, the learning\n",
    "            rate will start decreasing exponentially every epoch after the cycle with decay\n",
    "            factor `gamma`. In `constant` mode, the learning rate will be set to `const_lr`\n",
    "            after the cycle and will remain constant. Default: ``linear``.\n",
    "        min_momentum (int or float): Minimum momentum value within the range [0, 1]. Used only if\n",
    "            `cyclic_momentum` is ``True``. Default: 0.85.\n",
    "        cyclic_momentum (bool): Whether to vary the momentum value during the cycle. If\n",
    "            ``True``, the optimizer must have ``momentum`` parameter (or ``betas`` for Adam)\n",
    "            and the momentum value will linearly decrease to `min_momentum` and increase back\n",
    "            to the original value during the cycle. Default: ``False``.\n",
    "        last_epoch (int or float): The index of last epoch. Can be a float depending on your\n",
    "            purpose. Default: -1.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, max_lr, cycle_length, total_epochs=0, gamma=0.8, const_lr=3e-3, min_lr=0.,\n",
    "                 lr_mode_after_cycle=\"linear\", min_momentum=0.85, cyclic_momentum=False, last_epoch=-1):\n",
    "        if not isinstance(optimizer, optim.Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        if not isinstance(max_lr, (int, float)):\n",
    "            raise TypeError('expected an int or float for `max_lr`, but {} was given'.format(type(max_lr)))\n",
    "        if max_lr < 0:\n",
    "            raise ValueError('expected a non-negative value for `max_lr`, but {} was given'.format(max_lr))\n",
    "        self.max_lr = float(max_lr)\n",
    "        \n",
    "        if not isinstance(min_lr, (int, float)):\n",
    "            raise TypeError('expected an int or float for `min_lr`, but {} was given'.format(type(min_lr)))\n",
    "        if min_lr < 0:\n",
    "            raise ValueError('expected a non-negative value for `min_lr`, but {} was given'.format(min_lr))\n",
    "        self.min_lr = float(min_lr)\n",
    "        \n",
    "        if not isinstance(cycle_length, (int, float)):\n",
    "            raise TypeError('expected an int or float for `cycle_length`, but {} was given'.format(type(cycle_length)))\n",
    "        if cycle_length < 0:\n",
    "            raise ValueError('`cycle_length` must be non-negative')\n",
    "        self.cycle_length = cycle_length\n",
    "        \n",
    "        if cyclic_momentum:\n",
    "            if not isinstance(min_momentum, (int, float)):\n",
    "                raise TypeError('expected an int or float for `min_momentum`, but {} was given'.format(type(min_momentum)))\n",
    "            if not 0 <= min_momentum <= 1:\n",
    "                raise ValueError('expected a value within the range [0, 1] for `min_momentum`, but {} was given'.format(min_momentum))\n",
    "            self.cyclic_momentum = True\n",
    "            self.min_momentum = float(min_momentum)\n",
    "        else:\n",
    "            self.cyclic_momentum = False\n",
    "            self.min_momentum = None\n",
    "        \n",
    "        if not isinstance(lr_mode_after_cycle, str):\n",
    "            raise TypeError('expected a str for `lr_mode_after_cycle`, but {} was given'.format(type(lr_mode_after_cycle)))\n",
    "        self.lr_mode_after_cycle = lr_mode_after_cycle.lower()\n",
    "        if self.lr_mode_after_cycle == 'linear':\n",
    "            if not isinstance(total_epochs, (int, float)):\n",
    "                raise TypeError('expected an int or float for `total_epochs`, but {} was given'.format(type(total_epochs)))\n",
    "            self.total_epochs = total_epochs\n",
    "            self.gamma = gamma\n",
    "            self.const_lr = const_lr\n",
    "        elif self.lr_mode_after_cycle == 'exponential':\n",
    "            if not isinstance(gamma, (int, float)):\n",
    "                raise TypeError('expected a float for `gamma`, but {} was given'.format(type(gamma)))\n",
    "            if not 0 <= gamma <= 1:\n",
    "                raise ValueError('expected a value within the range [0, 1] for `gamma`, but {} was given'.format(gamma))\n",
    "            self.total_epochs = total_epochs\n",
    "            self.gamma = float(gamma)\n",
    "            self.const_lr = const_lr\n",
    "        elif self.lr_mode_after_cycle == 'constant':\n",
    "            if not isinstance(const_lr, (int, float)):\n",
    "                raise TypeError('expected a float for `const_lr`, but {} was given'.format(type(const_lr)))\n",
    "            if const_lr < 0:\n",
    "                raise ValueError('expected a non-negative value for `const_lr`, but {} was given'.format(const_lr))\n",
    "            self.total_epochs = total_epochs\n",
    "            self.gamma = gamma\n",
    "            self.const_lr = float(const_lr)\n",
    "        else:\n",
    "            raise ValueError('expected one of (`linear`, `exponential`, `constant`), but `{}` was given'.format(self.lr_mode_after_cycle))\n",
    "        \n",
    "        if last_epoch == -1:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if cyclic_momentum:\n",
    "                    if 'momentum' not in group:\n",
    "                        if 'betas' not in group:\n",
    "                            raise KeyError(\"param 'momentum' or 'betas' is not present \"\n",
    "                                           \"in param_groups[{}] of the given optimizer {}\".format(i, type(optimizer).__name__))\n",
    "                        elif self.min_momentum > group['betas'][0]:\n",
    "                            raise ValueError(\"first beta value in `betas` of param_groups[{}] of the given optimizer {} \"\n",
    "                                             \"is below `min_momentum` given\".format(i, type(optimizer).__name__))\n",
    "                        else:\n",
    "                            group.setdefault('initial_momentum', group['betas'][0])\n",
    "                    elif self.min_momentum > group['momentum']:\n",
    "                        raise ValueError(\"`momentum` value in param_groups[{}] of the given optimizer {} \"\n",
    "                                         \"is below `min_momentum` given\".format(i, type(optimizer).__name__))\n",
    "                    else:\n",
    "                        group.setdefault('initial_momentum', group['momentum'])\n",
    "                if self.max_lr < group['lr']:\n",
    "                    raise ValueError(\"`lr` value in param_groups[{}] of the given optimizer {} \"\n",
    "                                     \"exceeds `max_lr` given\".format(i, type(optimizer).__name__))\n",
    "                else:\n",
    "                    group.setdefault('initial_lr', group['lr'])\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
    "                if self.max_lr < group['initial_lr']:\n",
    "                    raise ValueError(\"`initial_lr` value in param_groups[{}] of the given optimizer {} \"\n",
    "                                     \"exceeds `max_lr` given\".format(i, type(optimizer).__name__))\n",
    "                if cyclic_momentum:\n",
    "                    if 'initial_momentum' not in group:\n",
    "                        raise KeyError(\"param 'initial_momentum' is not specified \"\n",
    "                                       \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
    "        self.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "        if cyclic_momentum:\n",
    "            self.base_momentums = list(map(lambda group: group['initial_momentum'], optimizer.param_groups))\n",
    "        else:\n",
    "            self.base_momentums = None\n",
    "        self.step(last_epoch + 1)\n",
    "        self.last_epoch = last_epoch\n",
    "        \n",
    "    def get_lr(self):\n",
    "        if self.last_epoch <= 0:\n",
    "            return self.base_lrs\n",
    "        elif self.last_epoch <= self.cycle_length:\n",
    "            return [self.max_lr + (base_lr - self.max_lr)\n",
    "                    * abs(-2. * (self.last_epoch / self.cycle_length) + 1)\n",
    "                    for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            if self.lr_mode_after_cycle == 'linear':\n",
    "                if self.last_epoch < self.total_epochs:\n",
    "                    return [max(base_lr *\n",
    "                                (1 - (self.last_epoch - self.cycle_length)\n",
    "                                 / (self.total_epochs - self.cycle_length)), self.min_lr)\n",
    "                            for base_lr in self.base_lrs]\n",
    "                else:\n",
    "                    return [self.min_lr for base_lr in self.base_lrs]\n",
    "            elif self.lr_mode_after_cycle == 'exponential':\n",
    "                return [max(base_lr * self.gamma ** (self.last_epoch - self.cycle_length), self.min_lr)\n",
    "                        for base_lr in self.base_lrs]\n",
    "            else:\n",
    "                return [max(self.const_lr, self.min_lr) for base_lr in self.base_lrs]\n",
    "        \n",
    "    def get_momentum(self):\n",
    "        if self.cyclic_momentum:\n",
    "            if 0 <= self.last_epoch < self.cycle_length:\n",
    "                return [self.min_momentum + (base_momentum - self.min_momentum)\n",
    "                        * abs(-2. * (self.last_epoch / self.cycle_length) + 1)\n",
    "                        for base_momentum in self.base_momentums]\n",
    "            else:\n",
    "                return self.base_momentums\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        \n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "        if self.cyclic_momentum:\n",
    "            for param_group, momentum in zip(self.optimizer.param_groups, self.get_momentum()):\n",
    "                if 'momentum' in param_group:\n",
    "                    param_group['momentum'] = momentum\n",
    "                else:\n",
    "                    param_group['betas'] = (momentum, param_group['betas'][1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
