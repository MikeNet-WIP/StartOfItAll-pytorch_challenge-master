{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 2.19701077e-08 9.45304756e-09 8.51529577e-09\n",
      " 2.69740258e-10]\n",
      "['78', '91', '88', '39', '73']\n"
     ]
    }
   ],
   "source": [
    "###################################LoadAllTheSHit\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "image_path = 'C:/PyTorch/flower_data/test/78/image_01874.jpg'\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = False\n",
    "\n",
    "#########################################\n",
    "\n",
    "model = models.vgg16(pretrained=False)\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "n_inputs = model.classifier[6].in_features\n",
    "\n",
    "last_layer = nn.Linear(n_inputs, 102)\n",
    "model.classifier[6] = last_layer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    #model.load_state_dict(torch.load(filepath, map_location='cpu'), strict=False)\n",
    "    \n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_checkpoint('BAMFFlowerPowerRevDv2True.pth')\n",
    "\n",
    "\n",
    "#########################\n",
    "\n",
    "def process_image(image_path):\n",
    "    ''' \n",
    "    Scales, crops, and normalizes a PIL image for a PyTorch       \n",
    "    model, returns an Numpy array\n",
    "    '''\n",
    "    # If you used something other than 224x224 cropped images, set the correct size here\n",
    "    #image_size = 224\n",
    "    # Values you used for normalizing the images. Default here are for \n",
    "    # pretrained models from torchvision.\n",
    "    #norm_mean = [0.485, 0.456, 0.406]\n",
    "    #norm_std = [0.229, 0.224, 0.225]\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Resize\n",
    "    if img.size[0] > img.size[1]:\n",
    "        img.thumbnail((50000, 256))\n",
    "    else:\n",
    "        img.thumbnail((256, 50000))\n",
    "\n",
    "    # Crop \n",
    "    left_margin = (img.width-224)/2\n",
    "    bottom_margin = (img.height-224)/2\n",
    "    right_margin = left_margin + 224\n",
    "    top_margin = bottom_margin + 224\n",
    "    img = img.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
    "    \n",
    "    # Normalize\n",
    "    img = np.array(img)/255\n",
    "    mean = np.array([0.485, 0.456, 0.406]) #Required mean\n",
    "    std = np.array([0.229, 0.224, 0.225]) #Required std\n",
    "    img = (img - mean)/std\n",
    "    \n",
    "    # Move color channels PyTorch expects the color channel to be the first dimension\n",
    "    #img = img.numpy().transpose((2, 0, 1))\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    \n",
    "    return img\n",
    "########################################################\n",
    "   \n",
    "## doneabove\n",
    "\n",
    "def imshow(image, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "##################################################\n",
    "\n",
    "def predict(image_path, model, topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    \n",
    "    # TODO: Implement the code to predict the class from an image file\n",
    "    # Process image\n",
    "    img = process_image(image_path)    \n",
    "    # Numpy -> Tensor\n",
    "    image_tensor = torch.from_numpy(img).type(torch.FloatTensor)\n",
    "    # Add batch of size 1 to image\n",
    "    model_input = image_tensor.unsqueeze(0)    \n",
    "    # Probs\n",
    "    probs = torch.exp(model.forward(model_input))    \n",
    "    # Top probs\n",
    "    top_probs, top_labs = probs.topk(topk)\n",
    "    top_probs_sum = np.sum(top_probs.detach().numpy())\n",
    "    top_probs = top_probs.detach().numpy().tolist()[0] \n",
    "    top_labs = top_labs.detach().numpy().tolist()[0]    \n",
    "    # Convert indices to classes\n",
    "    idx_to_class = {val: key for key, val in model.class_to_idx.items()}\n",
    "    top_labels = [idx_to_class[lab] for lab in top_labs]\n",
    "\n",
    "    \n",
    "    return top_probs/top_probs_sum, top_labels\n",
    "\n",
    "#######################################\n",
    "\n",
    "probs, classes = predict(image_path, model.cpu())\n",
    "\n",
    "print(probs)\n",
    "print(classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
