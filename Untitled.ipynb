{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '10', '100', '101', '102', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n"
     ]
    }
   ],
   "source": [
    "# define training and test data directories\n",
    "data_dir = 'C:/PyTorch/flower_data/'\n",
    "#data_dir = 'C:/PyTorch/flower_photos/'\n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "valid_dir = os.path.join(data_dir, 'valid/')\n",
    "test_dir = os.path.join(data_dir, 'test/')\n",
    "\n",
    "dirs = {'train': train_dir, \n",
    "        'valid': valid_dir, \n",
    "        'test' : test_dir}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(dirs[x],   transform=data_transforms[x]) for x in ['train', 'valid', 'test']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=True) for x in ['train', 'valid', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) \n",
    "                              for x in ['train', 'valid', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"1\",\n",
      "    \"10\",\n",
      "    \"100\",\n",
      "    \"101\",\n",
      "    \"102\",\n",
      "    \"11\",\n",
      "    \"12\",\n",
      "    \"13\",\n",
      "    \"14\",\n",
      "    \"15\",\n",
      "    \"16\",\n",
      "    \"17\",\n",
      "    \"18\",\n",
      "    \"19\",\n",
      "    \"2\",\n",
      "    \"20\",\n",
      "    \"21\",\n",
      "    \"22\",\n",
      "    \"23\",\n",
      "    \"24\",\n",
      "    \"25\",\n",
      "    \"26\",\n",
      "    \"27\",\n",
      "    \"28\",\n",
      "    \"29\",\n",
      "    \"3\",\n",
      "    \"30\",\n",
      "    \"31\",\n",
      "    \"32\",\n",
      "    \"33\",\n",
      "    \"34\",\n",
      "    \"35\",\n",
      "    \"36\",\n",
      "    \"37\",\n",
      "    \"38\",\n",
      "    \"39\",\n",
      "    \"4\",\n",
      "    \"40\",\n",
      "    \"41\",\n",
      "    \"42\",\n",
      "    \"43\",\n",
      "    \"44\",\n",
      "    \"45\",\n",
      "    \"46\",\n",
      "    \"47\",\n",
      "    \"48\",\n",
      "    \"49\",\n",
      "    \"5\",\n",
      "    \"50\",\n",
      "    \"51\",\n",
      "    \"52\",\n",
      "    \"53\",\n",
      "    \"54\",\n",
      "    \"55\",\n",
      "    \"56\",\n",
      "    \"57\",\n",
      "    \"58\",\n",
      "    \"59\",\n",
      "    \"6\",\n",
      "    \"60\",\n",
      "    \"61\",\n",
      "    \"62\",\n",
      "    \"63\",\n",
      "    \"64\",\n",
      "    \"65\",\n",
      "    \"66\",\n",
      "    \"67\",\n",
      "    \"68\",\n",
      "    \"69\",\n",
      "    \"7\",\n",
      "    \"70\",\n",
      "    \"71\",\n",
      "    \"72\",\n",
      "    \"73\",\n",
      "    \"74\",\n",
      "    \"75\",\n",
      "    \"76\",\n",
      "    \"77\",\n",
      "    \"78\",\n",
      "    \"79\",\n",
      "    \"8\",\n",
      "    \"80\",\n",
      "    \"81\",\n",
      "    \"82\",\n",
      "    \"83\",\n",
      "    \"84\",\n",
      "    \"85\",\n",
      "    \"86\",\n",
      "    \"87\",\n",
      "    \"88\",\n",
      "    \"89\",\n",
      "    \"9\",\n",
      "    \"90\",\n",
      "    \"91\",\n",
      "    \"92\",\n",
      "    \"93\",\n",
      "    \"94\",\n",
      "    \"95\",\n",
      "    \"96\",\n",
      "    \"97\",\n",
      "    \"98\",\n",
      "    \"99\"\n",
      "]\n",
      "{\n",
      "    \"21\": \"fire lily\",\n",
      "    \"3\": \"canterbury bells\",\n",
      "    \"45\": \"bolero deep blue\",\n",
      "    \"1\": \"pink primrose\",\n",
      "    \"34\": \"mexican aster\",\n",
      "    \"27\": \"prince of wales feathers\",\n",
      "    \"7\": \"moon orchid\",\n",
      "    \"16\": \"globe-flower\",\n",
      "    \"25\": \"grape hyacinth\",\n",
      "    \"26\": \"corn poppy\",\n",
      "    \"79\": \"toad lily\",\n",
      "    \"39\": \"siam tulip\",\n",
      "    \"24\": \"red ginger\",\n",
      "    \"67\": \"spring crocus\",\n",
      "    \"35\": \"alpine sea holly\",\n",
      "    \"32\": \"garden phlox\",\n",
      "    \"10\": \"globe thistle\",\n",
      "    \"6\": \"tiger lily\",\n",
      "    \"93\": \"ball moss\",\n",
      "    \"33\": \"love in the mist\",\n",
      "    \"9\": \"monkshood\",\n",
      "    \"102\": \"blackberry lily\",\n",
      "    \"14\": \"spear thistle\",\n",
      "    \"19\": \"balloon flower\",\n",
      "    \"100\": \"blanket flower\",\n",
      "    \"13\": \"king protea\",\n",
      "    \"49\": \"oxeye daisy\",\n",
      "    \"15\": \"yellow iris\",\n",
      "    \"61\": \"cautleya spicata\",\n",
      "    \"31\": \"carnation\",\n",
      "    \"64\": \"silverbush\",\n",
      "    \"68\": \"bearded iris\",\n",
      "    \"63\": \"black-eyed susan\",\n",
      "    \"69\": \"windflower\",\n",
      "    \"62\": \"japanese anemone\",\n",
      "    \"20\": \"giant white arum lily\",\n",
      "    \"38\": \"great masterwort\",\n",
      "    \"4\": \"sweet pea\",\n",
      "    \"86\": \"tree mallow\",\n",
      "    \"101\": \"trumpet creeper\",\n",
      "    \"42\": \"daffodil\",\n",
      "    \"22\": \"pincushion flower\",\n",
      "    \"2\": \"hard-leaved pocket orchid\",\n",
      "    \"54\": \"sunflower\",\n",
      "    \"66\": \"osteospermum\",\n",
      "    \"70\": \"tree poppy\",\n",
      "    \"85\": \"desert-rose\",\n",
      "    \"99\": \"bromelia\",\n",
      "    \"87\": \"magnolia\",\n",
      "    \"5\": \"english marigold\",\n",
      "    \"92\": \"bee balm\",\n",
      "    \"28\": \"stemless gentian\",\n",
      "    \"97\": \"mallow\",\n",
      "    \"57\": \"gaura\",\n",
      "    \"40\": \"lenten rose\",\n",
      "    \"47\": \"marigold\",\n",
      "    \"59\": \"orange dahlia\",\n",
      "    \"48\": \"buttercup\",\n",
      "    \"55\": \"pelargonium\",\n",
      "    \"36\": \"ruby-lipped cattleya\",\n",
      "    \"91\": \"hippeastrum\",\n",
      "    \"29\": \"artichoke\",\n",
      "    \"71\": \"gazania\",\n",
      "    \"90\": \"canna lily\",\n",
      "    \"18\": \"peruvian lily\",\n",
      "    \"98\": \"mexican petunia\",\n",
      "    \"8\": \"bird of paradise\",\n",
      "    \"30\": \"sweet william\",\n",
      "    \"17\": \"purple coneflower\",\n",
      "    \"52\": \"wild pansy\",\n",
      "    \"84\": \"columbine\",\n",
      "    \"12\": \"colt's foot\",\n",
      "    \"11\": \"snapdragon\",\n",
      "    \"96\": \"camellia\",\n",
      "    \"23\": \"fritillary\",\n",
      "    \"50\": \"common dandelion\",\n",
      "    \"44\": \"poinsettia\",\n",
      "    \"53\": \"primula\",\n",
      "    \"72\": \"azalea\",\n",
      "    \"65\": \"californian poppy\",\n",
      "    \"80\": \"anthurium\",\n",
      "    \"76\": \"morning glory\",\n",
      "    \"37\": \"cape flower\",\n",
      "    \"56\": \"bishop of llandaff\",\n",
      "    \"60\": \"pink-yellow dahlia\",\n",
      "    \"82\": \"clematis\",\n",
      "    \"58\": \"geranium\",\n",
      "    \"75\": \"thorn apple\",\n",
      "    \"41\": \"barbeton daisy\",\n",
      "    \"95\": \"bougainvillea\",\n",
      "    \"43\": \"sword lily\",\n",
      "    \"83\": \"hibiscus\",\n",
      "    \"78\": \"lotus lotus\",\n",
      "    \"88\": \"cyclamen\",\n",
      "    \"94\": \"foxglove\",\n",
      "    \"81\": \"frangipani\",\n",
      "    \"74\": \"rose\",\n",
      "    \"89\": \"watercress\",\n",
      "    \"73\": \"water lily\",\n",
      "    \"46\": \"wallflower\",\n",
      "    \"77\": \"passion flower\",\n",
      "    \"51\": \"petunia\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('cat_to_name.json', 'r') as f:\n",
    "    label_map = json.load(f)\n",
    "    \n",
    "print(json.dumps(class_names, sort_keys=False, indent=4, separators=(',', ': ')))\n",
    "print(json.dumps(label_map, sort_keys=False, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-5-68a4733a684a>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-68a4733a684a>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg19(pretrained=True)\n",
    "#model\n",
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(25088, 4096)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(4096, 102)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.classifier = classifier\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criteria, optimizer, scheduler,    \n",
    "                                      num_epochs=25, device='cuda'):\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criteria NLLLoss which is recommended with Softmax final layer\n",
    "criteria = nn.NLLLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optim = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 4 epochs\n",
    "sched = lr_scheduler.StepLR(optim, step_size=4, gamma=0.1)\n",
    "\n",
    "# Number of epochs\n",
    "eps=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model, criteria, optim, sched, eps, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
